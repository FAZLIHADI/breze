{
 "metadata": {
  "name": "FastDropout on MNIST"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import gzip\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "import theano.tensor as T\n",
      "\n",
      "import climin.stops\n",
      "import climin.initialize\n",
      "import climin.project\n",
      "import climin.schedule\n",
      "\n",
      "from brummlearn.mlp import Mlp, DropoutMlp, dropout_optimizer_conf, FastDropoutNetwork\n",
      "from breze.component.meanvartransfer import rectifier\n",
      "from breze.component.loss import nce\n",
      "\n",
      "from brummlearn.data import one_hot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prepare Data\n",
      "============"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datafile = '../examples/mnist.pkl.gz'\n",
      "# Load data.                                                                                                   \n",
      "\n",
      "with gzip.open(datafile,'rb') as f:                                                                        \n",
      "    train_set, val_set, test_set = cPickle.load(f)                                                       \n",
      "\n",
      "X, Z = train_set                                                                                               \n",
      "VX, VZ = val_set\n",
      "TX, TZ = test_set\n",
      "\n",
      "Z = one_hot(Z, 10)\n",
      "VZ = one_hot(VZ, 10)\n",
      "TZ = one_hot(TZ, 10)\n",
      "\n",
      "image_dims = 28, 28"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define Model\n",
      "============"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "\n",
      "def sampling_softmax(axis=1, rng=None):\n",
      "    if rng is None:\n",
      "        rng = RandomStreams()\n",
      "\n",
      "    def inner(mean, var):\n",
      "        # First we generate lots of standard normally distributed samples and\n",
      "        # transform them to (mean, var) distributed afterwards.\n",
      "        samples = rng.normal(size=mean.shape)\n",
      "        samples = samples * T.sqrt(var) + mean\n",
      "\n",
      "        # We then softmax each sample and return the mean of it.\n",
      "        samples -= samples.min(axis=axis).dimshuffle(0, 'x')\n",
      "\n",
      "        exped = T.exp(samples)\n",
      "        normalizer = exped.sum(axis=axis)\n",
      "        if axis == 1:\n",
      "            result = exped / normalizer.dimshuffle(0, 'x')\n",
      "        if axis == 2:\n",
      "            result = exped / normalizer.dimshuffle(0, 1, 'x')\n",
      "        return result, T.ones_like(var)\n",
      "\n",
      "    return inner\n",
      "\n",
      "softmax = sampling_softmax()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "max_passes = 1000\n",
      "batch_size = 250\n",
      "max_iter = max_passes * X.shape[0] / batch_size\n",
      "n_report = X.shape[0] / batch_size\n",
      "\n",
      "stop = climin.stops.any_([\n",
      "    climin.stops.after_n_iterations(max_iter),\n",
      "    ])\n",
      "\n",
      "pause = climin.stops.modulo_n_iterations(n_report)\n",
      "\n",
      "#softmax = sampling_softmax()\n",
      "\n",
      "loss = lambda target, prediction: nce(target, prediction[:, :prediction.shape[1] // 2])\n",
      "\n",
      "optimizer = 'rmsprop', {'steprate': 0.0001, 'momentum': 0.95, 'decay': 0.9, 'step_adapt': 0.0}\n",
      "#optimizer = dropout_optimizer_conf(steprate_0=1, n_repeats=1)\n",
      "#optimizer = 'lbfgs', {'n_factors': 25}\n",
      "\n",
      "#optimizer = 'gd', {'steprate': 1e-2, 'momentum': climin.schedule.sutskever_blend(0.995, 10), 'momentum_type': 'nesterov'}\n",
      "\n",
      "m = FastDropoutNetwork(784, [800, 800], 10, \n",
      "                       hidden_transfers=['rectifier', 'rectifier'], out_transfer=softmax,\n",
      "                       loss=loss, \n",
      "                       optimizer=optimizer, batch_size=batch_size, max_iter=max_iter, \n",
      "                       p_dropout_inpt=.2, p_dropout_hidden=.5)\n",
      "m.parameters.data[...] = np.random.normal(0, 1e-2, m.parameters.data.shape)\n",
      "\n",
      "weight_decay = ((m.parameters.in_to_hidden**2).sum()\n",
      "                + (m.parameters.hidden_to_hidden_0**2).sum()\n",
      "                + (m.parameters.hidden_to_out**2).sum())\n",
      "weight_decay /= m.exprs['inpt'].shape[0]\n",
      "m.exprs['true_loss'] = m.exprs['loss']\n",
      "c_wd = 0.0000\n",
      "m.exprs['loss'] = m.exprs['loss'] + c_wd * weight_decay\n",
      "\n",
      "f_wd = m.function(['inpt'], c_wd * weight_decay)\n",
      "n_wrong = 1 - T.eq(T.argmax(m.exprs['output_in_mean'] , axis=1), T.argmax(m.exprs['target'], axis=1)).mean()\n",
      "f_n_wrong = m.function(['inpt', 'target'], n_wrong)\n",
      "                \n",
      "losses = []\n",
      "v_losses = []\n",
      "print 'max iter', max_iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "max iter 200000\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Learn\n",
      "====="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time.time()\n",
      "# Set up a nice printout.\n",
      "keys = '#', 'loss', 'val loss', 'seconds', 'wd', 'train emp', 'val emp', 'test emp'\n",
      "max_len = max(len(i) for i in keys)\n",
      "header = '\\t'.join(i for i in keys)\n",
      "print header\n",
      "print '-' * len(header)\n",
      "\n",
      "f_loss = m.function(['inpt', 'target'], ['true_loss', 'loss'])\n",
      "\n",
      "for i, info in enumerate(m.powerfit((X, Z), (VX, VZ), stop, pause)):\n",
      "    if info['n_iter'] % n_report != 0:\n",
      "        continue\n",
      "    passed = time.time() - start\n",
      "    losses.append(info['loss'])\n",
      "    v_losses.append(info['val_loss'])\n",
      "    \n",
      "    #img = tile_raster_images(fe.parameters['in_to_hidden'].T, image_dims, feature_dims, (1, 1))\n",
      "    #save_and_display(img, 'filters-%i.png' % i)  \n",
      "    info.update({\n",
      "        'time': passed,\n",
      "        'l2-loss': f_wd(X),\n",
      "        'train_emp': f_n_wrong(X, Z),\n",
      "        'val_emp': f_n_wrong(VX, VZ),\n",
      "        'test_emp': f_n_wrong(TX, TZ),\n",
      "    })\n",
      "    row = '%(n_iter)i\\t%(loss)g\\t%(val_loss)g\\t%(time)g\\t%(l2-loss)g\\t%(train_emp)g\\t%(val_emp)g\\t%(test_emp)g' % info\n",
      "    print row\n",
      "    climin.project.max_length_columns(m.parameters['in_to_hidden'], 15)\n",
      "    climin.project.max_length_columns(m.parameters['hidden_to_hidden_0'], 15)\n",
      "    climin.project.max_length_columns(m.parameters['hidden_to_out'], 15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_predict = m.function(['inpt'], T.argmax(m.exprs['output_in'], axis=1))\n",
      "\n",
      "TY = f_predict(TX)\n",
      "\n",
      "print '#wrong', (TY != TZ.argmax(axis=1)).sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}